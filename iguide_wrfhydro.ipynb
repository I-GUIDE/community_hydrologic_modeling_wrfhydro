{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1233150b-c4ba-4057-9639-106d7ae36cd4",
   "metadata": {},
   "source": [
    "<img src=\"statics/iguide_logo.png\" width=200 height=200 />\n",
    "\n",
    "# National Water Model (WRFHydro) - Community Hydrological Modelling support on I-GUIDE platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f609a77-8dc4-4865-bf9c-f89a8847a309",
   "metadata": {},
   "source": [
    "The [I-GUIDE](https://iguide.illinois.edu/) project is extending and enhancing cyberinfrastructure that enables [National Water Model (NWM)](https://water.noaa.gov/about/nwm) data products to be configured and used to address specific questions related to water availability, flood prediction, and inundation mapping. The NWM is an implementation of the Weather Research and Forecasting Model Hydrological modeling system ([WRF-Hydro](https://ncar.ucar.edu/)) at continental-scale. The cyberinfrastructure and workflows needed to implement, set up and configure the data product subsets for a modeling domain of interest are complex, and demand a high level of skill from researchers, thus limiting broad applicability. I-GUIDE is building on modeling capability developed by the [HydroShare](https://www.hydroshare.org/), [CyberGISX](https://cybergisxhub.cigi.illinois.edu/) and other projects enabling computing for the hydrology community and integrating data connectors and processors from the [GeoEDF](https://github.com/geoedf) project to deploy simple to use workflows for setting up and running WRF-Hydro configured like the NWM for any watershed specified in the US NWM modeling domain.\n",
    "\n",
    "This notebook demostrates the setup for a typical WRFHydro model on I-GUIDE platform leveraging different tools or services through out the entire end-to-end modelling workflow illustrated below.\n",
    "\n",
    "<center><img src=\"statics/wrfhydro.png\" width=800 height=600 /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e4c27-7e8e-40a4-b5e6-d80b51380230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # allows us to communicate with the operating system\n",
    "import time # this module provides various time-related functions\n",
    "from datetime import datetime, timedelta # to specify the time domain for analysis\n",
    "import shutil # this module offers a number of high-level operations on files and collections of files\n",
    "import requests # request module lets us make request to web pages\n",
    "import xarray as xr #xarray makes it easier to work with multidimensional datasets like the NWM forecasts.\n",
    "import matplotlib.pyplot as plt  # helps to create plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55b5ca",
   "metadata": {},
   "source": [
    "## Setup Simulation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ba18c-da13-4d76-9c6e-4c9c39f060ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huc12 id\n",
    "# huc12 = \"070200030503\"\n",
    "huc12 = \"160102030302\"  # huc 12 to identify the spatial domain of interest\n",
    "# huc12 = \"160102030301\"\n",
    "# Start at 00:00 (12AM)\n",
    "start_datetime = datetime(2016, 1, 1)     # specify the simulation start time  (Year, Month, Day)\n",
    "#                          Y   M   D\n",
    "# End at 00:00 (12AM)\n",
    "end_datetime = datetime(2016, 1, 7)     # specify the simulation end time  (Year, Month, Day)\n",
    "#                        Y   M   D\n",
    "\n",
    "# version WRFHydro codebase on github (tag/release/commit id)\n",
    "wrfhydro_version = \"v5.2.0\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f009be-88f9-4518-804d-e4f94a11652d",
   "metadata": {},
   "source": [
    "## Map Layout\n",
    "\n",
    "This map layout represents the Logan River Watershed. For this example, we have selected the upstream watershed (the highlighted one)\n",
    "\n",
    "<img src=\"statics//Layout.jpg\" width=400 height=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc2b1c-a3ba-438c-9780-fe82d180e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_subset_domain = {\"huc12_id\": huc12, \n",
    "                        \"start_date\": start_datetime.strftime(\"%m/%d/%Y\"), \n",
    "                        \"end_date\": end_datetime.strftime(\"%m/%d/%Y\")}\n",
    "params_subset_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293edf59-fa23-4835-82ab-f7fe71e7d974",
   "metadata": {},
   "source": [
    "## Subset DOMAIN Files with GeoEDF Data Connector on CyberGIS Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195cebe0-f046-4fc9-b7bc-169866211e6d",
   "metadata": {},
   "source": [
    "The source of WRFHydro DOMAIN files is [CUAHSI Domain Subsetter](https://subset.cuahsi.org/) service. I-GUIDE provides a reusable [GeoEDF Data Connector](https://dl.acm.org/doi/10.1145/3311790.3396631) ([CUAHSISubsetterInput-Connector](https://github.com/I-GUIDE/cybergis-compute-cuahsisubsetterinput-connector) ) that makes requests to CUAHSI Domain Subsetter REST APIs and retrieves the domain files ready for model use. The GeoEDF Data Connector has been integrated into [CyberGIS-Compute](https://cybergis.github.io/cybergis-compute-python-sdk/reference.html) as a job that can be invoked by users from Jupyter environment and executed on supported HPC resources. The subset domain files staged remotely is ready for use by WRFHydro model on HPC, and user has the option to download the files from HPC back to Jupyter for local manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cybergis_compute_client\n",
    "from cybergis_compute_client import CyberGISCompute\n",
    "\n",
    "cybergis = CyberGISCompute(url=\"cgjobsup.cigi.illinois.edu\", isJupyter=True, protocol=\"HTTPS\", port=443, suffix=\"v2\")\n",
    "cybergis.show_ui(defaultJob=\"CUAHSI_Subsetter_Connector\", input_params=params_subset_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a78851-c2bc-4d88-ba8e-17b7844aa9d7",
   "metadata": {},
   "source": [
    "Make sure to download the output and then continueRetain Domain Subsetter JobID for later reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid_cuahsi_subset_domain = cybergis.job.id\n",
    "jobid_cuahsi_subset_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e2c4f-c6c7-44e4-a918-09ccd1960316",
   "metadata": {},
   "source": [
    "## Subset FORCINGS with GeoEDF Data Processor on CyberGIS Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f1265-fefd-4163-8e24-5964eefa6282",
   "metadata": {},
   "source": [
    "The source of WRFHydro FORCING files is [AORC](https://registry.opendata.aws/nwm-archive/) dataset hosted on AWS. I-GUIDE provides a reusable [GeoEDF Data Processor](https://dl.acm.org/doi/10.1145/3311790.3396631) ([SubsetForcingData-Processor](https://github.com/I-GUIDE/cybergis-compute-subsetaorcforcingdata-processor) ) that subsets forcing files spatially and temporally. This GeoEDF Data Processor has been integrated into [CyberGIS-Compute](https://cybergis.github.io/cybergis-compute-python-sdk/reference.html) as a job that can be invoked by users from Jupyter environment and executed on supported HPC resources. The subset forcing files staged remotely is ready for use by WRFHydro model on HPC, and user has the option to download the files from HPC back to Jupyter for local manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd3436-1624-4fcb-a02c-4f5554acacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cybergis_compute_client\n",
    "from cybergis_compute_client import CyberGISCompute\n",
    "\n",
    "cybergis = CyberGISCompute(url=\"cgjobsup.cigi.illinois.edu\", isJupyter=True, protocol=\"HTTPS\", port=443, suffix=\"v2\")\n",
    "cybergis.show_ui(defaultJob=\"Subset_AORC_Forcing_Data_Processor\", input_params=params_subset_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca565355-50c7-4688-9a0e-4442d6269571",
   "metadata": {},
   "source": [
    "Retain Forcing Processor JobID for later reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f50725-0878-4e41-bd84-1d1669c705d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid_subset_forcing = cybergis.job.id\n",
    "jobid_subset_forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c202e-dfe2-4898-aed2-19b50c3a4a6b",
   "metadata": {},
   "source": [
    "## Prepare Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cac2671-b7f6-4d89-9fd2-ae53a2c476ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setEnvar.sh ---> SPATIAL_SOIL=1 export HYDRO_D=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2713c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"Simulation\" directory\n",
    "\n",
    "workspace = os.getcwd()\n",
    "\n",
    "simulation_dir = os.path.join(workspace, 'Simulation')\n",
    "if os.path.exists(simulation_dir):\n",
    "    shutil.rmtree(simulation_dir)\n",
    "os.makedirs(simulation_dir)\n",
    "\n",
    "#List of files\n",
    "os.listdir(simulation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3486717-1af3-430e-9f26-7273ab6aefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/NCAR/wrf_hydro_nwm_public/v5.2.0/trunk/NDHMS/template/setEnvar.sh\n",
    "! sed -i '/export HYDRO_D=0/c\\export HYDRO_D=1' ./setEnvar.sh\n",
    "! sed -i '/export SPATIAL_SOIL=0/c\\export SPATIAL_SOIL=1' ./setEnvar.sh\n",
    "! cat ./setEnvar.sh | grep -E 'HYDRO_D|SPATIAL_SOIL'\n",
    "! mv ./setEnvar.sh {simulation_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56f547-b5ac-4c11-9ee5-1f32bee122df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# namelist.hrldas --> START_YEAR START_MONTH START_DAY START_HOUR START_MIN RESTART_FILENAME_REQUESTED\n",
    "start_year = start_datetime.year\n",
    "start_month = \"{:02d}\".format(start_datetime.month)\n",
    "start_day = \"{:02d}\".format(start_datetime.day)\n",
    "start_hour = \"{:02d}\".format(start_datetime.hour)\n",
    "start_minute = \"{:02d}\".format(start_datetime.minute)\n",
    "khour = (end_datetime - start_datetime) / timedelta(hours=1)\n",
    "khour = \"{}\".format(int(khour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f2e74-73c3-4a05-b07a-aae0fa55a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf namelist.hrldas\n",
    "! wget https://raw.githubusercontent.com/NCAR/wrf_hydro_nwm_public/v5.2.0/trunk/NDHMS/template/NoahMP/namelist.hrldas\n",
    "! sed -i  '/HRLDAS_SETUP_FILE/c\\HRLDAS_SETUP_FILE = \"./DOMAIN/wrfinput_d0x.nc\"' ./namelist.hrldas\n",
    "! sed -i  '/START_YEAR/c\\START_YEAR = '\"$start_year\" ./namelist.hrldas\n",
    "! sed -i  '/START_MONTH/c\\START_MONTH = '\"$start_month\" ./namelist.hrldas\n",
    "! sed -i  '/START_DAY/c\\START_DAY = '\"$start_day\" ./namelist.hrldas\n",
    "! sed -i  '/START_HOUR/c\\START_HOUR = '\"$start_hour\" ./namelist.hrldas\n",
    "! sed -i  '/START_MIN/c\\START_MIN = '\"$start_minute\" ./namelist.hrldas\n",
    "! sed -i  '/KHOUR =/c\\KHOUR = '\"$khour\" ./namelist.hrldas\n",
    "! sed -i  '/RESTART_FILENAME_REQUESTED/c\\!RESTART_FILENAME_REQUESTED = \"\"' ./namelist.hrldas\n",
    "! cat ./namelist.hrldas | grep -E 'HRLDAS_SETUP_FILE|START_|KHOUR'\n",
    "! mv ./namelist.hrldas {simulation_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff1dbbb-bec1-4dfa-85ad-f5471ee7c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydro.namelist  --> RESTART_FILE\n",
    "! rm -rf hydro.namelist\n",
    "! wget https://raw.githubusercontent.com/NCAR/wrf_hydro_nwm_public/v5.2.0/trunk/NDHMS/template/HYDRO/hydro.namelist\n",
    "! sed -i '/GEO_STATIC_FLNM/c\\GEO_STATIC_FLNM = \"./DOMAIN/geo_em.d0x.nc\"' ./hydro.namelist\n",
    "! sed -i '/RESTART_FILE/c\\!RESTART_FILE = \"\"' ./hydro.namelist\n",
    "! sed -i  '/GW_RESTART/c\\GW_RESTART = 0' ./hydro.namelist\n",
    "! sed -i  '/LSMOUT_DOMAIN/c\\LSMOUT_DOMAIN = 1' ./hydro.namelist\n",
    "! sed -i  '/output_gw/c\\output_gw = 1' ./hydro.namelist\n",
    "! sed -i '/outlake/c\\outlake  = 0' ./hydro.namelist\n",
    "! sed -i '/output_gw/c\\output_gw  = 0' ./hydro.namelist\n",
    "! sed -i '/GWBASESWCRT/c\\GWBASESWCRT  = 1' ./hydro.namelist\n",
    "! sed -i '/output_channelBucket_influx/c\\output_channelBucket_influx  = 2' ./hydro.namelist\n",
    "! sed -i '/channel_option/c\\channel_option  = 2' ./hydro.namelist\n",
    "! sed -i '/route_link_f/c\\route_link_f  = \"./DOMAIN/Route_Link.nc\"' ./hydro.namelist\n",
    "! sed -i '/compound_channel/c\\!compound_channel  = .FALSE.' ./hydro.namelist\n",
    "! sed -i '/route_lake_f/c\\!route_lake_f  = \"./DOMAIN/LAKEPARM.nc\"' ./hydro.namelist\n",
    "! sed -i '/gwbasmskfil/c\\!gwbasmskfil  = \"./DOMAIN/GWBASINS.nc\"' ./hydro.namelist\n",
    "! sed -i '/UDMP_OPT/c\\UDMP_OPT  = 1' ./hydro.namelist\n",
    "! sed -i '/!udmap_file/c\\udmap_file = \"./DOMAIN/spatialweights.nc\"' ./hydro.namelist\n",
    "! cat hydro.namelist | grep -E \"RESTART|outlake|GWBASESWCRT|route_lake_f|gwbasmskfil\"\n",
    "! mv ./hydro.namelist {simulation_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d116bc-dfa3-41fe-ac3c-b42d0d3799cc",
   "metadata": {},
   "source": [
    "## Run WRFHydro Model on HPC with CyberGIS Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa142b-3cf8-4916-afb3-1212ac439f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_wrfhydro = {\"Model_Version\": wrfhydro_version,\n",
    "                   \"LSM_Type\": \"NoahMP\",\n",
    "                   \"Forcing_Path\": jobid_subset_forcing,\n",
    "                   \"Domain_Path\": jobid_cuahsi_subset_domain,\n",
    "                   \"Merge_Output\": \"True\"}\n",
    "params_wrfhydro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a5878-22b2-4fe3-adc0-ec5147ddf0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cybergis_compute_client\n",
    "from cybergis_compute_client import CyberGISCompute\n",
    "\n",
    "cybergis = CyberGISCompute(url=\"cgjobsup.cigi.illinois.edu\", isJupyter=True, protocol=\"HTTPS\", port=443, suffix=\"v2\")\n",
    "cybergis.create_job_by_ui(defaultJob=\"wrfhydro-5.x\", defaultDataFolder=simulation_dir,input_params=params_wrfhydro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae776-7b0d-4648-89ff-a00852a1f539",
   "metadata": {},
   "source": [
    "Download only the Outputs_Merged/CHRTOUT folder and retain WRFHydro JobID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e9e39-1117-4681-9d7b-4fbb32511dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobid_wrfhydro = cybergis.job.id\n",
    "output_wrfhydro = cybergis.recentDownloadPath\n",
    "!ls -LR {output_wrfhydro}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa8334-87b1-45a4-865b-355b08819087",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0dec5-95f4-40eb-8325-0de6d274d4e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path of the merged file of channel routing \"CHRTOUT_DOMAIN1_merged.nc\"\n",
    "ch_file = '{}/Outputs_Merged/CHRTOUT/CHRTOUT_DOMAIN1_merged.nc'.format(output_wrfhydro)\n",
    "print(ch_file)\n",
    "# path of the route link \"Rouet_Link.nc\"\n",
    "routelink ='/home/jovyan/globus_download_{}/Route_Link.nc'.format(jobid_cuahsi_subset_domain)\n",
    "print(routelink)\n",
    "\n",
    "# convert rouetlink to dataframe\n",
    "route_df = xr.open_dataset(routelink).to_dataframe() # convert routelink to dataframe\n",
    "route_df.gages = route_df.gages.str.decode('utf-8').str.strip()\n",
    "route_df # print out the routelink dataframe\n",
    "\n",
    "# Re-name the \"Route_Link.nc\" variables\n",
    "cols = ['order', 'link', 'gages', 'lat', 'lon', 'to', 'from']  # columns name of original routelink dataframe\n",
    "route_re_df = route_df[cols].sort_values(by=['order'])      # reduce the size of routelink dataframe to include only these columns ['order', 'link', 'gages', 'lat', 'lon', 'to', 'from']\n",
    "\n",
    "# rename the columns\n",
    "route_renm_df = route_re_df.rename(index=str, columns={\"order\": \"stream_order\",\n",
    "                                       \"link\": \"comid\",\n",
    "                                       \"from\": 'upstream_comid',\n",
    "                                       'to':'downstream_comid',\n",
    "                                       \"gages\": 'usgs_gageid',\n",
    "                                       \"lat\": 'lat-midpoint',\n",
    "                                       \"lon\": 'lon-midpoint'})\n",
    "route_renm_df.reset_index(inplace=True)\n",
    "# data.drop('linkDim', axis=1, inplace=True)\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "# Load channel routing data \"CHRTOUT_DOMAIN1_merged.nc\"\n",
    "channel_ds = xr.open_dataset(ch_file)\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# Reduce the size of the dataset to only the essential variables.\n",
    "reach_ds = channel_ds\n",
    "reach_ds = reach_ds[['streamflow',        # Streamflow \n",
    "                     'q_lateral',         # lateral inflow through reach\n",
    "                     'qSfcLatRunoff',     # runoff from terrain routing\n",
    "                     'qBucket',           # flux from groundwater (gw) bucket\n",
    "                     'qBtmVertRunoff',    # runoff from bottom of soil to bucket\n",
    "                     'order',\n",
    "                     'velocity',\n",
    "                     'Head',\n",
    "                     'elevation']]\n",
    "\n",
    "## clean unnecessary data\n",
    "reach_ds = reach_ds.reset_coords()\n",
    "reach_ds = reach_ds.drop(labels=['latitude','longitude'])\n",
    "reach_ds.attrs = {}\n",
    "\n",
    "route_renm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53930b64-d736-4740-a83b-f4bc20cfb71d",
   "metadata": {},
   "source": [
    "The next cell shows all the National Water Model (NWM)  reaches. Please click on the reach of interest that is located within your selected watershed domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8939d-52e5-4779-91d8-c3781251877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<html>\n",
    "<head></head>\n",
    "<body>\n",
    "<iframe title=\"NWM Reaches\" src=\"https://byu-hydroinformatics.github.io/csb-jr233/\" width=900 height=600>\n",
    "</iframe>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65431c4d-c45c-42fe-bd62-9607f128fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "reachid= 664168\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,6), sharex='col')\n",
    "\n",
    "reach_ds.sel(feature_id=reachid)['streamflow'].plot(ax=ax,\n",
    "                                                    label='Total Outflow ($m^3$/s)',\n",
    "                                                    color='red',\n",
    "                                                    linestyle='--')\n",
    "# plot settings\n",
    "plt.ylabel('Discharge, $m^3$/s')\n",
    "plt.xlabel(\"Time\")\n",
    "# plt.title(\"\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('Hydrograph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f801f-4dce-4e18-a725-2dd3ee44341b",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:iguide]",
   "language": "python",
   "name": "conda-env-iguide-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
